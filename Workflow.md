---
title: Workflow

---


# Sample Pre-processing

Large sequences (>5 G) deposited in the NCBI can be downloaded from ENA without needing to use the SRA Toolkit.


Bioproject PRJNA254808 contains all samples from station 6 and 10.
Bioproject PRJNA632347 contains samples from station 2 and 6.

Each of these reads has been saved in a Sequence Read Archive (SRA) file with the following identification keys:

```
* SRR1509790
* SRR1509792
* SRR1509793
* SRR1509794
* SRR1509796
* SRR1509797
* SRR1509798
* SRR1509799
* SRR1509800
```
 
---
## Sample Download

This process involves retrieving the download script generated by the platform itself. This script is downloaded to a personal computer and then uploaded to the CIBNOR server account using the program Cyberduck.


---
Before starting the download on the CIBNOR server, you can run the following command to check the contents of the script provided by ENA:

```
$ cat ena-file-download-selected-files-[....].sh
```

This command allows you to view the download links for each SRA of the bioproject.

To run the script, you must first grant permission with:

```
$ chmod +x ena-file-download-selected-files-[....].sh
```

Once this is done, the script can be executed with the following command to run it in the background, allowing you to continue using the terminal and preventing progress from being lost if the terminal is closed.

```
$ nohup ./ena-file-download-selected-files-20250529-0312.sh > out_salida_st10.txt 2>&1 &
```

To check the progress or any errors that may occur, you can consult the out_salida_st10.txt file:

```
$ cat out_salida_st10.txt
```

Alternatively, you can view only the last lines of the file:

```
$ tail -f out_salida_st10.txt
```


---
## FastQC Execution
The FastQC program, broadly speaking, allows for the analysis and understanding of the quality of reads in a sample. It generates a descriptive report with graphics and statistics.

---
A total of 29 paired-end samples were downloaded, resulting in 58 .fastq.gz files that will be analyzed one by one in an automated manner using FastQC.

The first step is to activate the conda environment where FastQC is located.

```
$ conda activate fastqc
```

Next, open and create a text file with nano to write the script instructions:

```
#!/bin/bash

# Create a results folder if it doesn't exist
mkdir -p reads_fastqc

# Run FastQC on all .fastq.gz files in the directory
for file in *.fastq.gz; do
    echo "Processing $file ..."
    fastqc "$file" --outdir reads_fastqc
done

echo "FastQC analysis completed."
```

Finally, grant execution permissions and run the script:

```
$ chmod +x run_fastqc.sh

$ nohup ./run_fastqc.sh > out_fastqc_untrimmed.txt  2>&1 &
```

Once FastQC has finished analyzing the files, we can download the .html files to our local computer using the Cyberduck program and open them in a browser.

Another option is to decompress the .zip files and view the report results. This method is not as visual, but it allows us to identify the tests that passed and failed in the FastQC analysis.

```
$ for filename in *.zip
> do
> unzip $filename
> done
```

First, the summary.txt files from each file are concatenated into a single document to check the status of the test with the grep command.

```
$ cat */summary.txt > ~/directory/fastqc_summaries.txt

$ grep FAIL fastqc_summaries.txt
```

Deactivate the fastqc conda environment with:

```
$ conda deactivate
```


---
## MultiQC Execution

Unlike the FastQC program, the MultiQC program generates a single report for all the files we want to analyze. It also provides a report with graphics and statistics that allows for comparing the information contained in each file.

---
Before starting the MultiQC execution, activate the conda environment where the program is located:

```
$ conda activate metagenomas
```
Again, follow the steps to create a script with nano, grant permissions with chmod +x, and run with nohup and &.

Script instructions:

```
#!/bin/bash

echo "Starting MultiQC analysis..."

multiqc reads_fastqc --outdir reads_multiqc

echo "MultiQC analysis completed."
```

A single .html file is generated, which can be downloaded to the local computer using Cyberduck to view its content and get a clear idea of the parameters to be used in the next step.

Deactivate the conda environment:

```
$ conda deactivate
```
---
## Trimmomatic Execution

Trimmomatic is a tool that allows for trimming and filtering lower-quality reads. This is an essential step for determining the results of taxonomic assignment and/or assembly.

---

The following parameters reported in Alexander, et al 2023 were considered:
```
ILLUMINACLIP:TruSeq3-PE.fa:2:30:7 \
    LEADING:2 \
    TRAILING:2 \
    SLIDINGWINDOW:4:2 \
    MINLEN:50
```
Before executing Trimmomatic, ensure you have the TruSeq3-PE.fa file or download it with wget from the GitHub page https://raw.githubusercontent.com/timflutre/trimmomatic/master/adapters/TruSeq3-PE.fa

The script for Trimmomatic:
```
#!/bin/bash

# Create output folder if it doesn't exist
mkdir -p trimmed2_fastq

# Path to the adapters file
ADAPTERS="TruSeq3-PE.fa"

# Loop over all _R1.fastq.gz files
for R1 in *_1.fastq.gz
do
  # Get the sample prefix (before _R1)
  PREFIX=$(basename "$R1" _1.fastq.gz)

  # Define the corresponding R2 file
  R2="${PREFIX}_2.fastq.gz"

  # Define output files
  OUT1_P="trimmed_fastq/${PREFIX}_1_paired.fastq.gz"
  OUT1_U="trimmed_fastq/${PREFIX}_1_unpaired.fastq.gz"
  OUT2_P="trimmed_fastq/${PREFIX}_2_paired.fastq.gz"
  OUT2_U="trimmed_fastq/${PREFIX}_2_unpaired.fastq.gz"

  echo "Processing sample: $PREFIX"

  # Run Trimmomatic with conservative parameters
  trimmomatic PE -threads 4 -phred33 \
    "$R1" "$R2" \
    "$OUT1_P" "$OUT1_U" \
    "$OUT2_P" "$OUT2_U" \
    ILLUMINACLIP:${ADAPTERS}:2:30:7 \
    LEADING:2 \
    TRAILING:2 \
    SLIDINGWINDOW:4:2 \
    MINLEN:50

  echo "Finished: $PREFIX"
  echo "--------------------------------------------"
done

echo "Trimmomatic analysis completed."
```

---
## MEGAHIT Execution

MEGAHIT is an NGS assembler that is optimized for metagenomes. It joins short fragments or reads, producing longer, continuous sequences called contigs.

---
For the execution of Megahit, two scripts with different contig lengths were created. This allows for a comparison of the number of contigs reconstructed at 1000 and 3000 bp.

The script will be as follows:

```
#!/bin/bash

# Directory of your trimmed FASTQ files
TRIMMED_FASTQ_DIR="/home/sfonseca/sta_10/trimmed2_fastq" # Keeping this path as in the last script


# Main directory for MEGAHIT assembly results
MEGAHIT_OUT_DIR="/home/sfonseca/sta_10/assembled3_contigs" # OUTPUT DIRECTORY, MODIFY WITH EACH CHANGE TO MIN_CONTIG_LENGTH!


# Directory for log files
MEGAHIT_LOGS="/home/sfonseca/sta_10/assembly3_logs" 

# Create base directories if they don't exist
mkdir -p "$MEGAHIT_OUT_DIR"
mkdir -p "$MEGAHIT_LOGS"

# MEGAHIT parameters, modify the MIN_CONTIG_LENGTH to the desired length, in this case 1000 bp

MEGAHIT_K_MERS="21,41,61,81,101,121,141"
MIN_CONTIG_LENGTH="1000"
NUM_THREADS="4"


# Get sample prefixes (e.g., SRRXXXXXXX)
SAMPLES=$(ls "$TRIMMED_FASTQ_DIR"/*_1_paired.fastq.gz | xargs -n1 basename | sed 's/_1_paired.fastq.gz//')

# Loop for each sample
for SAMPLE_PREFIX in $SAMPLES
do
echo "Starting assembly for: $SAMPLE_PREFIX"

  # Input and output paths for the current sample
  READ1="${TRIMMED_FASTQ_DIR}/${SAMPLE_PREFIX}_1_paired.fastq.gz"
  READ2="${TRIMMED_FASTQ_DIR}/${SAMPLE_PREFIX}_2_paired.fastq.gz"
  SAMPLE_OUTPUT_DIR="${MEGAHIT_OUT_DIR}/${SAMPLE_PREFIX}_assembly"
  SAMPLE_LOG_FILE="${MEGAHIT_LOGS}/${SAMPLE_PREFIX}_megahit.log"


  # Execute MEGAHIT
  megahit \
    -1 "$READ1" \
    -2 "$READ2" \
    -o "$SAMPLE_OUTPUT_DIR" \
    --min-contig-len "$MIN_CONTIG_LENGTH" \
    --presets meta-sensitive \
    --k-list "$MEGAHIT_K_MERS" \
    -t "$NUM_THREADS" \
    > "$SAMPLE_LOG_FILE" 2>&1

  echo "Finished: $SAMPLE_PREFIX"
done

echo "All assemblies have been processed."

```

---
## CCMetagen Execution

Descripcción

---
KMA script:
```
#!/bin/bash

# Este script alinea contigs de un ensamblaje contra una base de datos de referencia.


# Variables:
# Correr desde el directorio donde se encuentran los archivos de contigs (final.contigs.fa)
input_dir="."

# Directorio donde se guardarán los resultados de KMA
output_dir="../../03_KMA_res"

# Crea el directorio de salida si no existe
mkdir -p "$output_dir"

# Ruta a la base de datos de referencia
ref_db="/home/sfonseca/base_indexada/compress_refSeq/refseq_bf"


# Bucle para procesar cada archivo de contigs
for contig_file in "$input_dir"/*.fa; do
	# Extrae el nombre base del archivo para el nombre de salida
	base_name=$(basename "$contig_file" .fa)
	
	# Construye la ruta completa del archivo de salida
	output_prefix="$output_dir/$base_name"


# Ejecuta KMA con los contigs como entrada de extremo simple (-i)
kma -i "$contig_file" -o "$output_prefix" -t_db "$ref_db" -t 4 -mem_mode -and


done
```

Process the results with CCMetagen

CCMetagen script

```
#!/bin/bash

# Este script corre CCmetagen


# Variables:

# Correr desde el directorio donde se encuentran los archivos de salida de KMA 
input_dir="03_KMA_res"
output_dir="03_CCMETAGEN"
mkdir -p "$output_dir"

# Change to the input directory to ensure the files are found.
cd "$input_dir"

# --- Proceso de CCMetagen ---
echo "Iniciando el procesamiento con CCMetagen..."

# Itera sobre cada archivo que termina en .res en el directorio de entrada.
for f in *.res; do 

# Extraer el nombre base del archivo (e.g., sample1.res -> sample1)
    base_name=$(basename "$f" .res)

# Construir la ruta completa para el archivo de salida.
    out_file="../$output_dir/${base_name}.txt"

	echo "Procesando el archivo: $f"
	
 # Ejecutar el comando CCMetagen.py con los archivos de entrada y salida.
	CCMetagen.py -i "$f" -o "$out_file"
done

echo "¡Proceso de CCMetagen completado! Los resultados están en el directorio: $output_dir"
```